---
title: "Machine learning steps"
author: "Quinn Thomas"
date: "2023-03-11"
output: github_document
---

## Step 1: Obtain data

Read in data to memory.  In the course we have accessed csv, excel, and database forms located in the github repo, on websites, and through an API.

## Step 2: Pre-process data

### Split data: divide data into the training and test sets. 

- You will use the training set to fit your model and the testing set to evaluate the model performance.
- We only want to evaluate our model using data that the model has not used yet.  Otherwise we can only say that the model is good at fitting the data that was used to train it.  This is not helpful for understanding how it would perform as a tool generate new predictions.
- You want most of your data in the training set because machine learning models can be quite "hungry for data" (e.g, next a large dataset to find a good fit).
- A typical split is 80% training and 20% testing but there isn't a required split. 
- Data are randomly assigned to the two splits.
- Your data may have obvious groupings that you want to be equally represented in both the training and testing sets.  For example, if you have cats and dogs as a variable you are using to predict tail length, you may want to be sure that your training set is not randomly full of dogs because than means it may not predict the cats well in the testing set.  You can prevent this by defining the `strata` used when assigning the training and testing sets. 

### Recipes: Modify predictors/features/covariates/independent variables for analysis

  - Modify data set so that variables are correctly formatted for a particular model. For example, a linear regression requires groups to be dummy variables. Therefore, a column called "ecosystem" with two values: "forest" and "grass" would be converted to two columns: ecosystem_forest with a value of 0 or 1 and ecosystem_grass with a value of 0 and 1)
  - Removing highly correlated predictors
  - Rescaling predictor (e.g., converting to 0 mean and 1 standard deviation)
  - transforming predictors (e.g., log)

## Step 3: Specify model and workflow

A workflow combines the model and recipe in a single "object" that you can use for training, testing, and predicting new data.

  - Define model type: linear regression, regression tree, neutral net, etc.
  - Define model engine: particular R package (`lm`, `ranger`, etc.)
  - Define model mode: regression or classification
  - Define workflow: combine recipe and model definition

## Step 4: Train model

  - Tune hyper-parameters: hyper-parameters are configuration settings the govern how a particular ML method is fit to data. They are called "hyper" because "regular" parameters are the parameter within the model that are learned by the ML method.  For example, a method called "random forecast" requires a hyper-parameter that controls the minimum size of a regression tree that is allowed.  This parameter (called `min_n`) could be directly provided by you or could be tuned.  The tuning process involves repeatedly fitting the model using different values of the hyper-parameter and using the hyper-parameter values that yield the best fit to the data. Importantly: not all ML methods have hyper-parameter (e.g., linear regression using the `lm` engine does not have hyper-parameters).  We don't tune hyperparameters in this example. See `example-with-tuning.Rmd` for an extension of this application that includes hyperparameter tuning.
  
  - Fit model (using best hyper-parameter if they are tuned).  The model is fit to the training data.

## Step 5: Predict

  - Predict testing data using the model that was fit to the training data.  This step is critical because we only want to evaluate our model using data that the model has not used yet.  Otherwise we can only say that the model is good at fitting the data that was used to train it.  This is not helpful for understand how it would perform as a tool generate new predictions. 
  - Predictions are quite easy using the `predict()` function.
  
## Step 6: Evaluate model

  - It is important to use the appropriate metrics to evaluate how the model performs.  Some metrics only apply to classification problems and other only apply to regression problems. A list of metric types can be found [here](https://yardstick.tidymodels.org/articles/metric-types.html)
  - We will be focusing on root-mean squared error (`rmse`), a metric that subtracts each observation from the predictions, squares it, averages all the squared errors for all the data points, and then takes the square root of the mean squared error.  
  - R-squared (`rsq`) is another metric that we used in the lake ice module.
  
## Step 7: Deploy model

  - One of main points of using machine learning is to develop a tool that can be used with new predictors to predict data that wasn't involved in the training and testing.  These are data that have all the columns necessary to make predictions but lack data in the column you are trying to predict.  
  - This step is simple because it involve using the `predict()` function with the same trained model but with new data.
  
# Application: Predicting biomass in NEON data

In the example application, we are going to be using the vegetation carbon stock data that was used in the carbon stock model. Our goal is to predict the mean carbon stock of each plot in the NEON data.  This will allow us to build a model that could be used to predict carbon stocks at locations other than NEON and potentially provide a prediction tool for land-managers.

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
set.seed(100) #for random number generation
```

## Step 1: Obtain data

The data are from the National Ecological Observatory Network, the DayMet project (meteorology data), and NASA's MODIS satellite.  They have been combined to provide the woody vegetation carbon stocks for each NEON plot and the associated meteorology, land-use classification, and normalized vegetation index (from MODIS).

```{r}
biomass_data <- read_csv("data/neon_biomass.csv", show_col_types = FALSE)
```

In this example analysis, we are only going to be using three columns.  `plotID` is the identifier for the plot, `nlcdClass` is the land-cover classification of plot, and `plot_kgCm2` is the vegetation carbon stock of the plot (kgC/m2).  We are using to be using `nlcdClass` to predict `plot_kgCm2` for each `plotID`.

```{r}
biomass_data <- biomass_data |> 
  select(plotID, nlcdClass, plot_kgCm2)
```

## Step 2:  Pre-process data

### Split data into training/testing sets 

We are going to split the data into training and testing sets using the `initial_split` function.  `prop = 0.80` says to use 80% of the data in the training set. Since we want the training and test sets to have representation of all the types of `nlcdClass`, we include it as a `strata`.


```{r}
split <- initial_split(biomass_data, prop = 0.80, strata = nlcdClass)
```

Our split should reflect the 80/20 that we defined using `prop`

```{r}
split
```

To actually get the training and testing data we need to apply the `training()` and `testing()` functions to the split.

```{r}
train_data <- training(split)
test_data <- testing(split)
```

You can see that `train_data` is a data frame that we can work with.

```{r}
train_data
```

### Feature engineering using a recipe

 - requires starting with dataset that is used to provide the columns.
 - a formula with the dependent variable and the predictors.  If `.` is used as the predictors, that means use all columns other than the dependent variable.
 - Steps that modify the data
 
 We will use the following steps:
 - `step_rm` because we don't want to use the identifier of the plot in the fitting.
 - `step_other` because there are rare land-use classes that we want to lump together.  This creates an "other" class.
 - `step_dummy` because linear regression requires classes to be dummy variables. For example, a linear regression requires groups to be dummy variables. Therefore, a column called "ecosystem" with two values: "forest" and "grass" would be converted to two columns: ecosystem_forest with a value of 0 or 1 and ecosystem_grass with a value of 0 and 1)
 
 [Here are the different recipe steps used above](https://recipes.tidymodels.org/reference/index.html)

- [Add dummy variables](https://recipes.tidymodels.org/reference/index.html#step-functions-dummy-variables-and-encodings)
- [Filter rows and select columns](https://recipes.tidymodels.org/reference/index.html#step-functions-filters)

Here is our recipe:

```{r}
biomass_recipe <- train_data |> 
  recipe(plot_kgCm2 ~ . ) |> 
  step_rm(plotID) |>
  step_other(nlcdClass) |>
  step_dummy(nlcdClass)
```

The recipe should show the steps that will be performed when applying the recipe. Importantly, these steps have not yet been applied, we just have a recipe of what to do.

```{r}
biomass_recipe
```

## Step 3: Specify model, engine, and workflow

We need to create the model and engine that we will use. In this example, we are using `linear_reg` with the mode of `regression` (as opposed to `classification`). Setting the mode for a linear regression is actually not necessary because it only allows regressions but it is included here for completeness. 

The engine is `lm` because we are using the standard R function `lm`.  There are a ton of other functions for linear regression modeling that we could use.  They would be specified as a different engine.

```{r}
linear_mod <- 
  linear_reg(mode = "regression") |> 
  set_engine("lm")
```

You will see the model, mode, and engine in the model object

```{r}
linear_mod 
```

We now combine the model and the recipe together to make a workflow that can be used to fit the training and testing data. `workflow()` initiates the workflow and `add_model` and `add_recipe` add those components to the workflow.  Importantly, the workflow has not yet been applied, we just have description of what to do.

```{r}
biomass_wflow <-
  workflow() %>%
  add_model(linear_mod) %>%
  add_recipe(biomass_recipe)
```

You can see that the workflow object has all the components together

```{r}
biomass_wflow
```

## Step 4: Train model on Training Data

We will use the workflow object to train the model. We need to provide the workflow object and the dataset to the fit function to fit (i.e., train the model)

```{r}
biomass_fit <- biomass_wflow |> 
  fit(data = train_data)
```

You can see that the fit object is the workflow object + the results of the model fitting

```{r}
biomass_fit
```

## Step 5: Predict Test Data

Now we will predict the testing data using the model that was fit to the training data.

```{r}
predictions <- predict(biomass_fit, new_data = test_data)
```

The predictions are a single column called `.pred`

```{r}
predictions
```

We need to combine the `.pred` column with the testing data using the `bind_cols` function

```{r}
pred_test <- bind_cols(test_data, predictions)
```

Now we have a data frame with the prediction and all the predictors used to predict it and other identifying info (like `plotID`)

```{r}
pred_test
```

## Step 6: Evaluate model

We will evaluate the performance of our predictions of the testing data using two metrics (`rmse` and `rsq`).  The function `metric_set` defines the set of metric we will be using them.  It creates a function called `multi_metric()` that we will use to calculate the metrics.  We pipe in the predicted test data (`pred_test`) and tell the function that our truth (i.e., observed data) is the `plot_kgCm2` column and the predictions (i.e., `estimate`) is the `.pred` column

```{r}
multi_metric <- metric_set(rmse, rsq)

metric_table <- pred_test |> 
multi_metric(truth = plot_kgCm2, estimate = .pred)
```

The resulting table has the metrics for evaluation

```{r}
metric_table
```

## Step 7: Deploy model

The final step is to apply your model to predict new data. 

Now read in the new data.  
```{r}
submit_data <- read_csv("data/neon_biomass_new.csv", show_col_types = FALSE) 
submit_data <- submit_data |> 
  select(plotID, nlcdClass, plot_kgCm2)
```

You will notice that the plot_kgCm2 is all `NA` because you don't know what the carbon stock is of the plot.

```{r}
submit_data
```

As in "Step 5: Predict Test Data", use the model fitting on the training data (`biomass_fit`) to predict the new data.

```{r}
new_predictions <- predict(biomass_fit, new_data = submit_data)
```

Now create a data frame with the new data, predictions, and your name. 

**Be sure to add your name to the `mutate()`**

```{r}
submit_predicted <- bind_cols(submit_data, new_predictions) |>
  mutate(name = "ADD YOUR NAME HERE") |>
  select(plotID, name, .pred)
```

Write these predictions to a csv

```{r}
write_csv(submit_predicted, file = "predictions.csv")
```

# Assignment

Your assignment is divided into two R markdown files that use data that you have already seen in the previous modules.

1) `assignment-part-1.Rmd` challenges you to translate the linear regression that you did in the lake ice module follow the tidymodel machine learning format presented in this Rmd

2) `assignment-part-2.Rmd` challenges you to develop your own machine learning model to predict the carbon stock data from the forest carbon module.




