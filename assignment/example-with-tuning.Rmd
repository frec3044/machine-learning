---
title: "Machine learning steps with hyperparameter tuning"
author: "Quinn Thomas"
date: "2023-03-15"
output: github_document
---

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
set.seed(100)
```

This extends the example in `machine-learning-101.Rmd` to include hyperparameter tuning.

## Step 1: Obtain data

In this example, we are going to use the same NEON biomass data with an additional predictor (`lat`)

```{r}
biomass_data <- read_csv("data/neon_biomass.csv", show_col_types = FALSE) |> 
  select(plotID, nlcdClass, plot_kgCm2, lat)
```

## Step 2: Pre-process data

### Split data into training/testing sets

This step is the same as `machine-learning-101.Rmd`

```{r}
split <- initial_split(biomass_data, prop = 0.80, strata = nlcdClass)

train_data <- training(split)
test_data <- testing(split)

```

### Split training data into folds

To tune the hyperparameters of the model, we will divide the training data into "folds".

- Each fold has an analysis and assessment set. Think if the analysis set as the training set within the fold and the assessment set as the testing set.
- We will randomly create 10 different folds because a single fold may not be representative of the full training set and the size of the assessment set within a fold is relatively small.  
- Our choice of the best hyperparameters will be bases on the average performance across the 10 folds.

From running `?vfold_cv`:

> V-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into V groups of roughly equal size (called "folds"). A resample of the analysis data consists of V-1 of the folds while the assessment set contains the final fold. In basic V-fold cross-validation (i.e. no repeats), the number of resamples is equal to V.

```{r}
folds <- vfold_cv(train_data, v = 10)
```

You can see information about the folds in the `folds` object

```{r}
folds
```

### Feature engineering using a recipe

This step is the same as `machine-learning-101.Rmd`

```{r}
biomass_recipe <- biomass_data |> 
  recipe(plot_kgCm2 ~ . ) |> 
  step_rm(plotID) |>
  step_other(nlcdClass) |>
  step_dummy(nlcdClass)
```

## Step 3: Specify model and workflow

### Model 

We are going to use the random forest model to predict biomass. The function [`rand_forest`](https://parsnip.tidymodels.org/reference/rand_forest.html) defines the model.  

These are the arguments for the `rand_forest` model

```
rand_forest(
  mode = "unknown",
  engine = "ranger",
  mtry = NULL,
  trees = NULL,
  min_n = NULL
)
```

We will use [the ranger engine](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html)

`ranger::ranger()` fits a model that creates a large number of decision trees, each independent of the others. The final prediction uses all predictions from the individual trees and combines them.

For this engine, there are multiple modes: classification and regression

[Tuning Parameters](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html#tuning-parameters)

This model has 3 tuning hyper-parameters:

- `mtry`: Randomly Selected Predictors (type: integer, default: see below). mtry depends on the number of columns. The default in ranger::ranger() is floor(sqrt(ncol(x))).
- `trees`: Trees (type: integer, default: 500L)
- `min_n`: Minimal Node Size (type: integer, default: see below). min_n depends on the mode. For regression, a value of 5 is the default. For classification, a value of 10 is used.

We will tune two of them in our model (`mtry` and `min_n`).  Setting the hyper-parameter equal to `tune()` is a placeholder that says we plan to tune it.  We also set `num.threads = parallel::detectCores()` so that the tuning uses the power of your computer to the fullest.

```{r}
rand_for_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = parallel::detectCores()) |> 
  set_mode("regression")
```


Now look at the [preprocessing requirements](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html#preprocessing-requirements) for the ranger engine.  

From the description of the engine: 

> This engine does not require any special encoding of the predictors. Categorical predictors can be partitioned into groups of factor levels (e.g. {a, c} vs {b, d}) when splitting at a node. Dummy variables are not required for this model.


Do we need to modify our recipe above?

### Workflow

```{r}
biomass_wflow <- 
  workflow() %>%
  add_model(rand_for_mod) %>%
  add_recipe(biomass_recipe)
```  

## Step 4: Train model

An important rule when training a model is that you never use the training data to evaluate a model.  This is because ML models can be so good that they "learn" the data.  Therefore the error on the training data will, by "definition" be low.  Evaluation is always done on data that was not used in the model training.  We have two ways that we are doing this

1) We have divided the data into a training and testing set with 20% of the data in the testing set.  The model will never be fit to the testing set, it will only be used to predict this set.

2) We have divided the training set into 10 folds where each fold has an analysis and assessment set.  Each fold is randomly different that the other folds with different rows of the training assigned to the analysis and assessment set.  We can then train the model on the analysis set in each fold separately and evaluate using the assessment set.  We can then average the model performance from the 10 different assessments to get an idea of model performance even before we look at the testing data. We will use the folds to tune the model hyperparameters.

In summary: 

- all data is divided into training and testing sets. 
- The training set has 10 different grouping of training data.  Each group has an analysis and accessment set.

### Estimate best hyper-parameters using tuning

The `tune_grid()` function will train the model+recipe defined in the workflow using a set of parameters on a subset of the training data (analysis set).  It then calculates a metric that describes how well the model with that set of hyper-parameter predicts the the "assessment" set.  

Setting the grid to 25 creates 25 sets of the two different hyper-parameters (5 x 5) that we are tuning.  It uses a sensible range of hyper-parameters to develop the grid.  

We will be using the metric root-mean-squeared error to measure how good the model fit is (`metrics = metric_set(rmse)`)

`control = control_grid(save_pred = TRUE)` just says to save the predictions for each fold.


```{r message = FALSE}
biomass_resample_fit <- 
  biomass_wflow |>  
  tune_grid(resamples = folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

In total the random forest model was run 25 * 10 times (25 different hyperparameter sets x 10 different folds of the training data).

In the table below that is sorted so that best hyperparameter set (lowest RMSE) is at the top.  You can see the value for `mtry` and `min_n` that were used in the training and mean RMSE (`mean`) across all 10 "folds" (`n` is the number of folds).  

```{r}
biomass_resample_fit %>% 
  collect_metrics() |> 
  arrange(mean)
```

The `select_best()` function helps extract the hyperparameters with the best metric (`rmse`).  

```{r}
best_hyperparameters <- biomass_resample_fit %>%
  select_best("rmse")
```

Are these the same as the top row above?

```{r}
best_hyperparameters
```

### Update workflow with best hyper-parameters

Our workflow (model + recipe) needs to know the hyper-parameters to use to fit the model.  The `finalize_workflow` function updates the workflow to contain the best hyper-parameters.

```{r}
final_workflow <- 
  biomass_wflow %>% 
  finalize_workflow(best_hyperparameters)
```

### Fit to all training data

We use the same approach as we have used before to train the model using the **full** training data (does not use the 10 folds).

```{r}
biomass_fit <- final_workflow |> 
  fit(data = train_data)
```

## Step 5: Predict testing data

This step is the same as `machine-learning-101.Rmd`

```{r}
predictions <- predict(biomass_fit, test_data)
```

```{r}
pred_test <- bind_cols(test_data, predictions)
```

## Step 6: Evaluate model

This step is the same as `machine-learning-101.Rmd`

```{r}
multi_metric <- metric_set(rmse, rsq)

metric_table <- pred_test |> 
multi_metric(truth = plot_kgCm2, estimate = .pred)
```

```{r}
metric_table
```

## Step 7:  Deploy model

This step is the same as `machine-learning-101.Rmd`

```{r}
submit_data <- read_csv("data/neon_biomass_new.csv", show_col_types = FALSE) 
```

```{r}
new_predictions <- predict(biomass_fit, submit_data)
```

```{r}
submit_predicted <- bind_cols(submit_data, new_predictions) |>
  mutate(team_name = "ADD YOUR LAST NAME")
```

