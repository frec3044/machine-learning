---
title: "Assignment Part 2: new ML model"
author: "Add your name"
format:
  html:
    embed-resources: true
---

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
set.seed(100)
```

## Objective

Using machine learning and tidymodels to predict how much carbon is stored in vegetation across the U.S.

## Expections

This assignment requires the following

- You must use more predictors than used in the `machine-learning-101.Rmd` and `example-with-tuning.Rmd`.  You do not have to use all predictors.  In fact, one of the decisions that you will be making is which predictors you will include.
  
- You can't use `lm` as an engine (since you have already done this twice)

- You must upload your predictions of the new carbon stock data to Canvas as a CSV.
  
### Guidance

You will need to explore the different models/engines available in the tidy models package. The link below will show the list of available models. Clicking on a model type will show the different engines available to use.  

**[Alternative modeling approaches](https://parsnip.tidymodels.org/reference/index.html#models)**

Here is a list of models that may be particularly useful for the data in this assignment.

- [Single layer neural network](https://parsnip.tidymodels.org/reference/mlp.html)
- [Random forest](https://parsnip.tidymodels.org/reference/rand_forest.html)
- [Automatic Machine Learning](https://parsnip.tidymodels.org/reference/auto_ml.html)
- [Boosted trees](https://parsnip.tidymodels.org/reference/boost_tree.html)
- [Linear Regression](https://parsnip.tidymodels.org/reference/linear_reg.html)

The documentation of the engine will describe any pre-processesing that is required for the data and list the hyperparameters that are available for tuning. For example [here](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html) is the documentation for the `ranger` engine within the `rand_forest` model

Once you start pre-processing your data using a recipe, the following link will show the different "steps" that you can use.  Clicking on the step will show how to use it in a recipe.  

**[Different recipes steps for feature engineering](https://recipes.tidymodels.org/reference/index.html)**

There are broad classes of recipe steps that you may find helpful for the data that you are working within this assignment.  You will not be using all the available steps.

- [Imputation (fill NA values)](https://recipes.tidymodels.org/reference/index.html#step-functions-imputation)
- [Transform variables](https://recipes.tidymodels.org/reference/index.html#step-functions-individual-transformations)
- [Add dummy variables](https://recipes.tidymodels.org/reference/index.html#step-functions-dummy-variables-and-encodings)
- [Create variable interaction](https://recipes.tidymodels.org/reference/index.html#step-functions-interactions)
- [Normalize variables (e.g., center on 0 and scale sd)](https://recipes.tidymodels.org/reference/index.html#step-functions-normalization)
- [Filter rows and select columns](https://recipes.tidymodels.org/reference/index.html#step-functions-filters)
- [Operate on rows](https://recipes.tidymodels.org/reference/index.html#step-functions-row-operations)

## Step 0: Warm-up

**Question 11:** What model and engine did you select to use?

**Answer 11:**

**Question 12:** Provide a high level description of the modeling approach you choose in Question 11.

**Answer 12:**


**Question 13:** Does the model require any special data pre-processing? If so, describe the steps needed.

**Answer 13:**

**Question 14:** List any hyper-parameters that can be tuned

**Answer 14:**

## Step 1: Obtain data

```{r}
biomass_data <- read_csv("data/neon_biomass.csv", show_col_types = FALSE)
```

```
spc_tbl_ [1,000 Ã— 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
 $ plotID    : chr - identifier of plot
 $ daylength : num - mean annual daylength (s/day)
 $ precip    : num - mean annual precipitation (mm)
 $ range     : num - mean annual daily range in temperature (day max - day min; C)
 $ tavg      : num - mean annual temperature (C)
 $ solar     : num - mean annual solar radiation (W/m2)
 $ tmax      : num - mean annual daily max temperature (C)
 $ tmin      : num - mean annual daily min temperature (C)
 $ vpd       : num - mean annual vapor pressure deficient (Pa)
 $ elevation : num - elevation of plot (meters)
 $ nlcdClass : chr - land-cover classification of plot
 $ lat       : num - latitude of plot (degrees W)
 $ long      : num - longitude of plot (degrees E)
 $ plotType  : chr - type of plot (tower or distributed)
 $ ndvi      : num - normalized difference vegetation index of plot from MODIS (unitless)
 $ siteID    : chr - identifier of site that includes the plot
 $ plot_kgCm2: num - vegetation carbon stock of plot (kgC/m2)
```

## Step 2: Pre-process data

### Split data into training/testing sets

**Question 15:** Provide code for splitting data

**Answer 15:**

```{r}


```

### Split training data into folds

This step is only required if you are using a model and engine that has hyper-parameters that can be tuned.

**Question 16:** Provide code for splitting data into folds. If your model does not require parameter tuning, then please just go ahead and state that as your answer rather than providing the code.

**Answer 16:**

```{r}


```

### Feature engineering using a recipe

**Question 17:** Provide code that defines the recipe for feature engineering. Be sure to follow the recommendations for your selected engine. 

**Answer 17:**

```{r}


```


## Step 3: Specify model and workflow

### Define model type and mode

**Question 18:** Provide code that defines your model (model type + engine).

**Answer 18:**

```{r}


```

### Define workflow

**Question 19:** Provide code that defines the workflow

**Answer 19:**

```{r}


```

## Step 4: Train model on Training Data

### Estimate best hyper-parameters using tuning

**Question 20:** Provide code that tunes the hyper-parameters of your model.  If your model+engine does not require tuning, then state that as your answer.

**Answer 20:**

```{r}


```

### Update workflow with best hyper-parameters

**Question 21:** Provide code that updates the workflow with the best hyper-parameters. If your model+engine does not require tuning, then state that as your answer.

**Answer 21:**

```{r}


```

## Step 5: Fit to all training data

**Question 22:** Provide code that fits your model to the training data. Use the best hyper-parameter if you tuned them.

**Answer 22:**

```{r}


```

## Step 6: Predict Test Data

**Question 23:** Provide code that predicts the test data

**Answer 23:**

```{r}


```

## Step 7: Evaluate model

**Question 24:** Provide code that calculates the rmse and r2 for the test data.  Make sure that the table with `rmse` and `r2` is shown in your rendered document.

**Answer 24:**

```{r}


```

## Step 8: Deploy model

Use your trained model to predict new data. The new data is a data frame with all the same columns as the training and testing set above but with `NA` values for the biomass. I have the true biomass values and will calculate `rmse` for your predictions. Therefore you need to predict the biomass and upload your CSV to Canvas and I will download them as a set and run the analysis in class to examine the "scores"

```{r}
new_data <- read_csv("data/neon_biomass_new.csv", show_col_types = FALSE) 
```


**Question 25:** Provide code to predict the new data

**Answer 25:**

**Question 26:** Use `write_csv(submit_predicted, file = paste0(name,"-submission.csv"))` to write your submissions to a csv.

Be sure to change the `name` to your last name.

**Answer 26:**

```{r}
name <- "ADD YOUR LAST NAME HERE"
new_predicted <- bind_cols(new_data, new_predictions) |>
  mutate(name = name) |>
  select(plotID, name, .pred)
write_csv(new_predicted, file = paste0(name,"-submission.csv"))
```

**Question 27:** Find your CSV on your computer from Question 26 and upload to Canvas.

**Answer 27:**

## Knitting and committing

Remember to Render your document as an `HTML` and comment+push to GitHub your code and rendered document.

# Attribution

Include citation of any AI-generated assistance or discussion with classmates (per policy in syllabus). Proper documentation of AI-generated assistance includes the prompt, the source (e.g., ChatGPT), and the significant parts of the response.  Proper documentation of discussion with classmates include listing their names and the components discussed.  


