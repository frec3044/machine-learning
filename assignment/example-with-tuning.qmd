---
title: "Machine learning steps with hyperparameter tuning"
author: "Quinn Thomas"
format:
  html:
    embed-resources: true
---

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
set.seed(100)
```

This extends the example in `machine-learning-101.Rmd` to include hyperparameter tuning.

## Step 1: Obtain data

In this example, we are going to use the same NEON biomass data with an additional predictor (`lat`)

```{r}
biomass_data <- read_csv("data/neon_biomass.csv", show_col_types = FALSE) |> 
  select(plotID, nlcdClass, plot_kgCm2, lat)
```

## Step 2: Pre-process data

### Split data into training/testing sets

This step is the same as `machine-learning-101.Rmd`

```{r}
split <- initial_split(biomass_data, prop = 0.80, strata = nlcdClass)

train_data <- training(split)
test_data <- testing(split)

```

### Split training data into folds

To tune the hyperparameters of the model, we will divide the training data into "folds."

- Each fold has an analysis and assessment set. Think of the analysis set as the training set within the fold and the assessment set as the testing set.
- We will randomly create 10 different folds because a single fold may not be representative of the full training set and the size of the assessment set within a fold is relatively small.  
- Our choice of the best hyperparameters will be based on the average performance across the 10 folds.

From running `?vfold_cv`:

> V-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into V groups of roughly equal size (called "folds"). A resample of the analysis data consists of V-1 of the folds, while the assessment set contains the final fold. In basic V-fold cross-validation (i.e. no repeats), the number of resamples is equal to V.

```{r}
folds <- vfold_cv(train_data, v = 10)
```

You can see information about the folds in the `folds` object

```{r}
folds
```

### Feature engineering using a recipe

This step is the same as `machine-learning-101.Rmd`

```{r}
biomass_recipe <- biomass_data |> 
  recipe(plot_kgCm2 ~ . ) |> 
  step_rm(plotID) |>
  step_other(nlcdClass) |>
  step_dummy(nlcdClass)
```

## Step 3: Specify model and workflow

### Model 

We are going to use the random forest model to predict biomass. The function [`rand_forest`](https://parsnip.tidymodels.org/reference/rand_forest.html) defines the model.  

These are the arguments for the `rand_forest` model

```
rand_forest(
  mode = "unknown",
  engine = "ranger",
  mtry = NULL,
  trees = NULL,
  min_n = NULL
)
```

We will use [the ranger engine](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html)

`ranger::ranger()` fits a model that creates a large number of decision trees, each independent of the others. The final prediction combines all the predictions from the individual trees.

For this engine, there are multiple modes: classification and regression

[Tuning Parameters](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html#tuning-parameters)

This model has 3 tuning hyper-parameters:

- `mtry`: Randomly Selected Predictors (type: integer, default: see below). mtry depends on the number of columns. The default in `ranger::ranger()` is `floor(sqrt(ncol(x)))`.
- `trees`: Trees (type: integer, default: `500L`)
- `min_n`: Minimal Node Size (type: integer, default: see below). `min_n` depends on the mode. For regression, a value of 5 is the default. For classification, a value of 10 is used.

We will tune two of them in our model (`mtry` and `min_n`).  Setting the hyper-parameter equal to `tune()` is a placeholder that says we plan to tune it.  We also set `num.threads = parallel::detectCores()` so that the tuning uses the power of your computer to the fullest by assigns different model runs to different computer cores (laptops often have 4-8 cores).

```{r}
rand_for_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("ranger", num.threads = parallel::detectCores()) |> 
  set_mode("regression")
```


Now look at the [preprocessing requirements](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html#preprocessing-requirements) for the ranger engine.  

From the description of the engine: 

> This engine does not require any special encoding of the predictors. Categorical predictors can be partitioned into groups of factor levels (e.g. {a, c} vs {b, d}) when splitting at a node. Dummy variables are not required for this model.


Do we need to modify our recipe above?

### Workflow

```{r}
biomass_wflow <- 
  workflow() |> 
  add_model(rand_for_mod) |> 
  add_recipe(biomass_recipe)
```  

## Step 4: Train model

An important rule when training a model is that you never use the training data to evaluate a model.  This is because ML models can be so good that they "learn" the data.  Therefore, the error on the training data will, by "definition," be low.  Evaluation is always done on data that was not used in the model training.  We have two ways that we are doing this

1) We have divided the data into a training and testing set with 20% of the data in the testing set.  The model will never be fit to the testing set, it will only be used to predict this set.

2) We have divided the training set into 10 folds where each fold has an analysis and assessment set.  Each fold is randomly different from the other folds, with different rows of the training assigned to the analysis and assessment set.  We can then train the model on the analysis set in each fold separately and evaluate using the assessment set.  We can then average the model performance from the 10 different assessments to get an idea of model performance even before we look at the testing data. We will use the folds to tune the model hyperparameters.

In summary: 

- all data is divided into training and testing sets. 
- The training set has 10 different groups of training data.  Each group has an analysis and assessment set.

### Estimate best hyper-parameters using tuning

The `tune_grid()` function will train the model+recipe defined in the workflow using a set of parameters on a subset of the training data (analysis set).  It then calculates a metric that describes how well the model with that set of hyper-parameters predicts the "assessment" set.  

Setting the grid to 25 creates 25 sets of the two different hyper-parameters (5 x 5) that we are tuning.  The grid is developed using a sensible range of hyper-parameters.  

We will be using the metric root-mean-squared error to measure the quantification of the model fit (`metrics = metric_set(rmse)`)

`control = control_grid(save_pred = TRUE)` says to save the predictions for each fold.


```{r message = FALSE}
biomass_resample_fit <- 
  biomass_wflow |>  
  tune_grid(resamples = folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

In total, the random forest model was run 25 * 10 times (25 different hyperparameter sets x 10 different folds of the training data).   This is why we used `num.threads = parallel::detectCores()` because it assigns different model runs to different computer cores (laptops often 4-8 cores).

The table below is sorted so that the best hyperparameter set (lowest RMSE) is at the top. You can see the values for `mtry` and `min_n` that were used in the training and the mean RMSE (`mean`) across all 10 "folds" (`n` is the number of folds).  

```{r}
biomass_resample_fit |>  
  collect_metrics() |> 
  arrange(mean)
```

The `select_best()` function extracts the hyperparameters with the best metric (`rmse`).  

```{r}
best_hyperparameters <- biomass_resample_fit |> 
  select_best(metric = "rmse")
```

Are these the same as the top row above?

```{r}
best_hyperparameters
```

### Update workflow with best hyper-parameters

Our workflow (model + recipe) needs to know the hyper-parameters to use to fit the model.  The `finalize_workflow` function updates the workflow so that it contains the best hyper-parameters.

```{r}
final_workflow <- 
  biomass_wflow |> 
  finalize_workflow(best_hyperparameters)
```

### Fit to all training data

We use the same approach as we have used before to train the model using the **full** training data (e.g., it does not use the 10 folds).

```{r}
biomass_fit <- final_workflow |> 
  fit(data = train_data)
```

## Step 5: Predict testing data

This step is the same as `machine-learning-101.Rmd`

```{r}
predictions <- predict(biomass_fit, test_data)
```

```{r}
pred_test <- bind_cols(test_data, predictions)
```

## Step 6: Evaluate model

This step is the same as `machine-learning-101.Rmd`

```{r}
multi_metric <- metric_set(rmse, rsq)

metric_table <- pred_test |> 
multi_metric(truth = plot_kgCm2, estimate = .pred)
```

```{r}
metric_table
```

## Step 7:  Deploy model

This step is the same as `machine-learning-101.Rmd`

```{r}
new_data <- read_csv("data/neon_biomass_new.csv", show_col_types = FALSE) 
```

```{r}
new_predictions <- predict(biomass_fit, new_data)
```

```{r}
new_predicted <- bind_cols(new_data, new_predictions) |>
  mutate(name = "ADD YOUR LAST NAME") |>
  select(plotID, name, .pred)
```

```{r}
write_csv(new_predicted, file = "predictions.csv")
```

